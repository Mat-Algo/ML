{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c601365",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f512483",
   "metadata": {},
   "source": [
    "Deep learning is a field within machine learning that deals with building and using neural network models.\n",
    "\n",
    "Neural network mimic the functioning of a human brain. NR with more than 3 layers are typically categorised as Deep Learning networks\n",
    "\n",
    "Deep Learning has been extremely popular in NLP because the neural network architectures are ideal for dealing with unstructured data. For the same reason, they are also popular for speech recognition.\n",
    "\n",
    "Image recognition is another domain where Deep Learning models are deployed. Self-driving cars..\n",
    "\n",
    "The applications of DL is popular in domains like customer experiance, health care and robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2003d",
   "metadata": {},
   "source": [
    "The concept of Linear Regression forms a key foundation for Deep Learning projects\n",
    "\n",
    "Linear Regression is a linear model that explains relationship between 2 or more variables. It is usually used to predict continuous variables. \n",
    "\n",
    "A related technique that is most used in Deep Learning is Logistic regression. It is a binary model that defines relationship between two variables.\n",
    "\n",
    "    Here the diff is that the LogR converts the LinR using sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7f4b9",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80beeb82",
   "metadata": {},
   "source": [
    "The preceptron is the unit for learning in an AI neural network. A perceptron represents the algorithm for supervised learning\n",
    "in an ANN. It resembles a human brain cell.  Multiple inputs are fed into perceptron which in turn does computations and outputs a boolean variable. It represents a single cell or node in a neural network. It is built based on logistic regression \n",
    "\n",
    "In DL we replace with weight and intercept with the bias called b. Weights and biases become the parameters for a NN. We then apply an acivation function f that outputs a boolean result based on the values.\n",
    "\n",
    "The number of weights = the number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10cb9f",
   "metadata": {},
   "source": [
    "A ANN is a network of perceptrons. A deep neural network usually has 3 or more layers and each node had its own weights, biases and activation function. Each node is connected to all the nodes in the next layer forming a dense network. The number of layers(hidden layers) and the number of nodes in each layer are determined by experience and trials and it changes from case to case.\n",
    "\n",
    "The inputs or independent variables are send from the input layer of the network. Data may be pre-processed before using them. Each node is a perceptron containing weights, bias and activation function. The formula is applied on the inputs and the outputs derived. As the process reaches the output layer the final predictions will be derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68ce27",
   "metadata": {},
   "source": [
    "An ANN is created through a model training process. A NN is represented by a set of parameters and hyperparameters. Training an ANN means determining the right values for these parameters and hyperparameters such that it maximises the accuracy of predictions for the given use case.\n",
    "\n",
    "We use training data like regular ML where we know both the dependent and independent variables. We start the network architecture by intuition . We also initialise weights and biases through random values. Then we repeat the iterations of applying weights and biases to the inputs and computing the error. Based on the error found we will adjust the weights and biases to reduce the error. We keep repeating the proces of adjusting weights and biases till the error gets to an acceptable value. We will also find tune the network hyperparameters to improve training speed and reduce iterations. Finally, we will save  the model as represented by its parameters and hyperparameters and then use it for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fb9da",
   "metadata": {},
   "source": [
    "## Neural Network acrhitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2b694",
   "metadata": {},
   "source": [
    "#### Input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8bc8c",
   "metadata": {},
   "source": [
    "A vector is an ordered list of numeric values. The input to deep learning model is usually a vector of numeric values. Vectors are usually defined using numpy arrays. It represents the filter variables or independent variables that are used for predictions as well as training. There are a number of advanced pre-processing techniques that are applied to data to prepare them for DL. \n",
    "\n",
    "Once the input data is ready it can be passed to the deep learning model for learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79778779",
   "metadata": {},
   "source": [
    "#### Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b15df",
   "metadata": {},
   "source": [
    "An ANN can have one or more hidden  layers. The more the number of layers the deeper the network is. Each hidden layer can have one or more nodes. Typically, the node count is confugured in the range of 2 power n. EG: count maybe 8,16,32,64,128,etc. The neural network architecture is defined by number of layers and nodes in that layer. The output of each node in previous layer will become the input for every node in the current layer. Similarly, the output of each node in the current layer is passed to every node in the next layer. \n",
    "When there are more nodes and layers it usually results in better accuracy. As a general practice, start with small number and keep adding until acceptable accuracy levels are obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4db959",
   "metadata": {},
   "source": [
    "#### Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea1812",
   "metadata": {},
   "source": [
    "These form the foundation for DL algorithms. Weights and biases are the trainable parameters in a NN model. During a training process the values for these weights and biases are determined such that they provide accurate predictions. Weight and biases are nothing but a collection of numeric values. Each input for each node will have an associated weight with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b12cef",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133cb4f",
   "metadata": {},
   "source": [
    "An activation function play an important role in creating the output of the node in the neural network. An activation function takes the matrix output of the node and determines if and how the node will propagate its information to the next layer. Activation functions act as filters to reduce noise and also normalise the output.\n",
    "\n",
    "The main adavantage is that it converts the output into a non-linear value. They serve as a critical step helping a neural network learn specific patterns in the data. \n",
    "\n",
    "TanH-- A TanH function normalizes the output in the range of -1 to +1. \n",
    "\n",
    "ReLu- Rectified Linear Unit---it produces a 0 if the output is negative, else it will reproduce the same input verbatim\n",
    "\n",
    "Softmax activation function is used in case of classification problems. It produces a vector of probabilities for each of the possible classes in the outcome .The sum pf probabilities will be equal to 1. The class with the highest probability will be considered for prediction.\n",
    "\n",
    "These all activation functions are added as hyperparameter to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13c14c",
   "metadata": {},
   "source": [
    "#### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918a912",
   "metadata": {},
   "source": [
    "The output layer is the final layer in the NN where desired predictions are obtained.\n",
    "\n",
    "There is one output layer in a NN that produces the desired final prediction. It has its own set of weights and biases that are appplied before the final output is derived. The activation function or the output layer may be different than the hidden layer based on the problem. For example - The Softmax activation function is used to derive the final classes in a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85032ae2",
   "metadata": {},
   "source": [
    "### Training a Neural Netork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8cd44",
   "metadata": {},
   "source": [
    "Setup and Initialization:- Applying a number of processing techniques to convert your samples into  numeric vectors. To help with training the input data is usually split into training, test and validation sets. A training data set is used to run through the neural network and fit the parameters like weights and biases. Once a model is created, the , the validation data set is used to check for its accuracy and error rates. The result from this validation is then used to refine a model and recheck. \n",
    "\n",
    "When a final model is obtained, it is used to predict on the test data set to measure the final model performance. The usual plot of input data between the training, validation and test set is 80:10:10. In order to create the initial model a set of values needs to be selected for various parameters and hyper-parameters. This includes the number of layers and the numer of nodes in each layer. We also need to select the activation functions for each layer. Then there are hyperparameters like epochs, batch sizes and error functions that needs to be selected. \n",
    "\n",
    "\n",
    "How do we make the initial selection? It is/maybe based on our own intuition and experience. It can also be based on references in best practices and suitability of techiniques to the specific problem. Whatever, values are selected, they are then redefined as the model is trained, if the final result of the model are not acceptable then we will go back adjust the parameters and then retrain the model. \n",
    "\n",
    "We also need to initialize the weights and biases for each nodes in the neural network. Multiple techniques for initialization are availible.\n",
    "\n",
    "- Zero Initialization - here we initialize all values to zero\n",
    "\n",
    "- The preferred technique is random initialization: here we initialize the weights and biases to random values obtained from a standard normal ditribution whose mean is 0 and standard deviation is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7cd2c",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45ce96",
   "metadata": {},
   "source": [
    "The input data is organised as samples and features. y is the actual value of the target in the training set, y_hat will be the values that will be predicted through forward propagation.\n",
    "\n",
    "The forward propagation step is exactly the same as doing an actual prediction with the neural network. For each sample, the inputs are send through the designated neural network. For each node, we compute the outputs based on the perceptron formula and pass them to the next layer.\n",
    "\n",
    "The final outcome y_hat is then obtained at the end. As we keep sending the samples to the neural network we will collect values of y_hat for each sample. This process is repeated for all samples in the training dataset. We will then compare the values of y_hat and y and compute the error rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f6acb",
   "metadata": {},
   "source": [
    "### Measuring accuracy and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9ac85",
   "metadata": {},
   "source": [
    "It is used to represent the gap between the predicted values and actual values of the target variable. For computing error we use 2 functiona, namely the loss function and the cost function.\n",
    "\n",
    "The loss predicts the error for a single sample while a cost function measures the error across a set of samples. The cost function provides an averaging effect over all the errors found on the trianing dataset. The terms loss function and cost function are used interchangeably. There are a number of popular cost function avalible. \n",
    "\n",
    "- MSE or RMSE - measures error in case of regression problem.\n",
    "\n",
    "- For binary classification we use Binary Cross Entropy to compute the error.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230d0c0",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43a286",
   "metadata": {},
   "source": [
    "Once we have estimated the prediction error from forward propagation, we need to go back to back propagation, to adjust the weights and biases. Back propagation works in the reverse direction as the forward propagation. When the batch size is equal to data size it is called batch gradient descent and when the batch size is less than training data size it is called mini batch gradient descent.\n",
    "\n",
    "A training data is sent through the neural network multiple times during the learning process. The total number of times the entire training data is sent through the neural network is called as epoch.. Higher epoch size may lead to better accuracy but it also delays the learning process.\n",
    "\n",
    "Lets say that we have a training data size 1000. We set the batch size as 128 and epoch count of 50. 1000/128 = 8.... The total iteration of passes through the neural network is a total batches multiplied by total epochs. In this case there will be a total of 400 passes through the neural network. This means the weights and biases are updated 400 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d72716",
   "metadata": {},
   "source": [
    "### Validation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bf656",
   "metadata": {},
   "source": [
    "As we build models we need to also validate and test them against independent datasets to measure out of sample error. The model can be fine-tuned and the learning process repeated based on the results seen against the validation dataset. After all the fine-tuning is completed and the final model is obtained, the test data is used to evaluate the model. This is done only once at the end . The evalutation results are then used to measure the performance of the model in terms of final accuracy and error rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccbdce7",
   "metadata": {},
   "source": [
    "## Deep Learning Example - Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa27548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\matth\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (0.25.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: setuptools in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (3.20.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d3653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data for DL \n",
    "# Load data into pandas dataframe\n",
    "# convert the dataframe to a numpy array\n",
    "# Scale the feature dataset\n",
    "# Use one-hot encoding for the targer variables (dependent variable)\n",
    "# Split in to training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99acdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b890e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  Species\n",
      "0           5.1          3.5           1.4          0.2        0\n",
      "1           4.9          3.0           1.4          0.2        0\n",
      "2           4.7          3.2           1.3          0.2        0\n",
      "3           4.6          3.1           1.5          0.2        0\n",
      "4           5.0          3.6           1.4          0.2        0\n",
      "\n",
      "Feature before scaling: \n",
      "-----------------------------------\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "Target before scaling: \n",
      "-----------------------------------\n",
      "[0. 0. 0. 0. 0.]\n",
      "\n",
      "Feature after scaling: \n",
      "-----------------------------------\n",
      "[[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]\n",
      " [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]\n",
      " [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]\n",
      " [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]\n",
      " [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]\n",
      "\n",
      "Target after scaling: \n",
      "-----------------------------------\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "Train test dimensions:\n",
      "-----------------------------------\n",
      "(135, 4) (15, 4) (135, 3) (15, 3)\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "iris_data = pd.read_csv(\"iris.csv\")\n",
    "print(iris_data.head())\n",
    "\n",
    "#use label encoder to convert string to numeric values for the target varible \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "iris_data['Species'] = label_encoder.fit_transform(iris_data['Species'])\n",
    "print(iris_data.head())\n",
    "\n",
    "#convert input into numpy array\n",
    "np_iris = iris_data.to_numpy()\n",
    "\n",
    "#separate feature and target variables \n",
    "X_data = np_iris[:,0:4]\n",
    "Y_data = np_iris[:,4]\n",
    "\n",
    "print(\"\\nFeature before scaling: \\n-----------------------------------\")\n",
    "print(X_data[:5,:])\n",
    "print(\"\\nTarget before scaling: \\n-----------------------------------\")\n",
    "print(Y_data[:5])\n",
    "\n",
    "#create a standard scaler model that is fit on the input data\n",
    "scaler = StandardScaler().fit(X_data)\n",
    "\n",
    "# scale the numeric feature variables\n",
    "X_data = scaler.transform(X_data)\n",
    "\n",
    "# convert target variable as a one hot encoding\n",
    "Y_data = tf.keras.utils.to_categorical(Y_data,3)\n",
    "\n",
    "print(\"\\nFeature after scaling: \\n-----------------------------------\")\n",
    "print(X_data[:5,:])\n",
    "print(\"\\nTarget after scaling: \\n-----------------------------------\")\n",
    "print(Y_data[:5])\n",
    "\n",
    "# Splitting training data and test data\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_data,Y_data,test_size=0.1)\n",
    "\n",
    "print(\"\\nTrain test dimensions:\\n-----------------------------------\")\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257c6702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a model\n",
    "# Number of hidden layers\n",
    "# Number of nodes in each layer\n",
    "# Activation functions\n",
    "# Loss function and accuracy measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf545432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden-layer-1 (Dense)      (None, 128)               640       \n",
      "                                                                 \n",
      " Hidden-layer-2 (Dense)      (None, 128)               16512     \n",
      "                                                                 \n",
      " Output-layer (Dense)        (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,539\n",
      "Trainable params: 17,539\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Number of classes in the target variable\n",
    "NB_CLASSES = 3\n",
    "\n",
    "                  \n",
    "\n",
    "# Create a sequential model keras \n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(keras.layers.Dense(128, #number of nodes\n",
    "                             input_shape = (4,), # number of input variables or number of features\n",
    "                             name =  \"Hidden-layer-1\", #logical name\n",
    "                             activation = 'relu')) # activation function \n",
    "           \n",
    "# Add a second hidden layer\n",
    "model.add(keras.layers.Dense(128,\n",
    "                            name = \"Hidden-layer-2\",\n",
    "                            activation = 'relu'))\n",
    "\n",
    "# Add an output layer with softmax function\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                            name = \"Output-layer\",\n",
    "                            activation = 'softmax'))\n",
    "\n",
    "# Compile the model with loss and metrics\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# Print the model meta-data\n",
    "model.summary()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6cd319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Progress: \n",
      "----------------------------------------\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 2s 98ms/step - loss: 0.8037 - accuracy: 0.7685 - val_loss: 0.7001 - val_accuracy: 0.7407\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.5134 - accuracy: 0.8426 - val_loss: 0.5786 - val_accuracy: 0.7778\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4064 - accuracy: 0.8519 - val_loss: 0.5105 - val_accuracy: 0.7778\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.3429 - accuracy: 0.8611 - val_loss: 0.4643 - val_accuracy: 0.8148\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.3046 - accuracy: 0.8704 - val_loss: 0.4356 - val_accuracy: 0.7778\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2697 - accuracy: 0.8796 - val_loss: 0.4184 - val_accuracy: 0.7778\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2416 - accuracy: 0.9167 - val_loss: 0.3951 - val_accuracy: 0.8148\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2217 - accuracy: 0.9167 - val_loss: 0.3785 - val_accuracy: 0.8519\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1951 - accuracy: 0.9167 - val_loss: 0.3607 - val_accuracy: 0.8889\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1788 - accuracy: 0.9352 - val_loss: 0.3956 - val_accuracy: 0.8148\n",
      "\n",
      "Accuracy during training: \n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation against Test Dataset: \n",
      "--------------------------\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3665 - accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.366523414850235, 0.9333333373069763]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and evaluating the model\n",
    "\n",
    "# Make it verbose so we can see the process\n",
    "VERBOSE = 1\n",
    "\n",
    "# Set hyperparameters for training\n",
    "# set batch size\n",
    "BATCH_SIZE = 16 \n",
    "# Set the number of epochs\n",
    "EPOCHS = 10\n",
    "# Set the validation split. 20% of the training data will be used for validation\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "print(\"\\nTraining Progress: \\n----------------------------------------\")\n",
    "\n",
    "# Fit the model. This will perform the entire training cycle, including forward propagation,\n",
    "# loss computation, backward propagation and gradient descent.\n",
    "# Execute for the specified batch sizes and epochs\n",
    "# Perform validation after each epoch\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   Y_train,\n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   epochs = EPOCHS,\n",
    "                   verbose = VERBOSE,\n",
    "                   validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "print(\"\\nAccuracy during training: \\n---------------------------------\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the accuracy of the model after each epoch\n",
    "pd.DataFrame(history.history)[\"accuracy\"].plot(figsize = (8,5))\n",
    "plt.title(\"Accuracy improvement with each epoch\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model against the test dataset and print result\n",
    "print(\"\\nEvaluation against Test Dataset: \\n--------------------------\")\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcccd026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Hidden-layer-1_input with unsupported characters which will be renamed to hidden_layer_1_input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: iris_save\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: iris_save\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden-layer-1 (Dense)      (None, 128)               640       \n",
      "                                                                 \n",
      " Hidden-layer-2 (Dense)      (None, 128)               16512     \n",
      "                                                                 \n",
      " Output-layer (Dense)        (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,539\n",
      "Trainable params: 17,539\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "model.save(\"iris_save\")\n",
    "\n",
    "#Loading the model\n",
    "loaded_model = keras.models.load_model(\"iris_save\")\n",
    "\n",
    "# print the model summary\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1964b00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Prediction Output (Probabilities): [[0.03882463 0.44173503 0.51944035]]\n",
      "Prediction is  ['virginica']\n"
     ]
    }
   ],
   "source": [
    "# Predictions with DL models\n",
    "\n",
    "# Raw prediction data\n",
    "prediction_input = [[6.6,3.5,4.4,1.8]]\n",
    "\n",
    "# scale production data with the same scaling data\n",
    "scaled_input = scaler.transform(prediction_input)\n",
    "\n",
    "# get raw prediction probabilities\n",
    "raw_prediction = model.predict(scaled_input)\n",
    "print(\"Raw Prediction Output (Probabilities):\",raw_prediction)\n",
    "\n",
    "# Find Prediction\n",
    "prediction = np.argmax(raw_prediction)\n",
    "print(\"Prediction is \",label_encoder.inverse_transform([prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de782709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
