{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518963e1",
   "metadata": {},
   "source": [
    "MNIST dataset\n",
    "\n",
    "CNN\n",
    "\n",
    "Keras is a high level neural network's API written in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fb3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import load_img, array_to_img\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51b4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8dadb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868f5724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understand the image format\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea9e032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2438a5e8fd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0],cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585cca5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c07b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image data\n",
    "image_height,image_width = 28,28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb50a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000,image_height*image_width)\n",
    "x_test  = x_test.reshape(10000,image_height*image_width)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73da42b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c93490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215687\n",
      " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
      " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.94509804\n",
      " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      " 0.5882353  0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.99215686\n",
      " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "x_train /=255.0\n",
    "x_test /= 255.0\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e819267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771f598",
   "metadata": {},
   "source": [
    "Converting the target value in to 10 bins. So we will see that the output form a model will then go in into one of these 10 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ff28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627a749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512,activation='relu',input_shape = (784,)))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(10,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10fae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04c5a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1839 - accuracy: 0.9436 - val_loss: 0.1015 - val_accuracy: 0.9691\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0806 - accuracy: 0.9742 - val_loss: 0.0815 - val_accuracy: 0.9755\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0553 - accuracy: 0.9826 - val_loss: 0.0975 - val_accuracy: 0.9718\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0413 - accuracy: 0.9871 - val_loss: 0.0815 - val_accuracy: 0.9772\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0347 - accuracy: 0.9892 - val_loss: 0.0842 - val_accuracy: 0.9785\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0283 - accuracy: 0.9905 - val_loss: 0.0870 - val_accuracy: 0.9796\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0238 - accuracy: 0.9926 - val_loss: 0.1086 - val_accuracy: 0.9780\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0238 - accuracy: 0.9928 - val_loss: 0.0896 - val_accuracy: 0.9830\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.0985 - val_accuracy: 0.9801\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0195 - accuracy: 0.9941 - val_loss: 0.1016 - val_accuracy: 0.9798\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0181 - accuracy: 0.9945 - val_loss: 0.0983 - val_accuracy: 0.9822\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0942 - val_accuracy: 0.9821\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0140 - accuracy: 0.9958 - val_loss: 0.1164 - val_accuracy: 0.9815\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 0.1400 - val_accuracy: 0.9789\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1463 - val_accuracy: 0.9780\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(x_train,y_train,epochs=15,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04535189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24389a3c400>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting accuracy of the model\n",
    "\n",
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8174871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1463 - accuracy: 0.9780\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "score = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd14f59",
   "metadata": {},
   "source": [
    "Fully connected neural networks typically don't work well on images. This is because each pixel is an input, then as we add morelayers the amount of parameters increases exponentially. 32* 32 That's a 32 high and 32 wide and with three color channels. A single fully connected neurom in the first layer of neural network  is 32 x 32 x 3 = 3072 weights. \n",
    "\n",
    "similarly for 120*120*3 = 120,000 weights\n",
    "\n",
    "The other challenge is that the number of parameters this large can quickly lead to over-fitting.\n",
    "\n",
    "The solution is CNN - The convolution is a mathematical operation used to extract features from an image. The convolution is defined by an image kernel. The image kernel is small matrix. Most of the time , a 3x3 kernel matrix is used . Image -> kernel -> convolved feature. \n",
    "\n",
    "Padding\n",
    "Pooling\n",
    "\n",
    "If a n*n matrix convolved with an f*f matrix with padding p then the size of the output image will be (n+2p-f+1)*(n+2p-f+1)\n",
    "\n",
    "Stride - it is the number of pixels shifts over the input matrix. For padding p ,filter f*f and input image of size n*n and stride s, the output image dimension will be  \n",
    "[(n+2p-f+1)/s+1] * [(n+2p-f+1)/s+1]\n",
    "\n",
    "Pooling  - A pooling layer is another building block of a CNN model. Pooling function is to progressively reduce the spatial size of the representation to reduce the network complexity and computational cost.\n",
    "\n",
    "Max Pooling  - it is simply a rule to take the maximum of a region and it helps to proceed with the most important features from the image. Max pooling selects the brighter pixels form the image. It is useful when the background of the image is dark and we are interested in only the lighter pixels of the image\n",
    "\n",
    "Average Pooling - It will consider the average of the region. Avg pooling blends all the values of a given region. \n",
    "\n",
    "The benefit of CNN is that it reduces the chance of your model over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0034be7",
   "metadata": {},
   "source": [
    "In neural network we only had fully connected layer, otherwise known as the dense layer. With CNN we have more operations such as the convolution operation max pooling, flattening and also a fully connected or dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0b6abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "\n",
    "from keras.layers import Conv2D,MaxPool2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92d9a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb8a2a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "545a042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(10000,28,28,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3585125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdb41d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                401472    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411,690\n",
      "Trainable params: 411,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# CNN Model Development\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32,kernel_size=(3,3), input_shape=(28,28,1),padding='same',activation='relu'))\n",
    "\n",
    "cnn.add(Conv2D(32,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "cnn.add(MaxPool2D())\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(64,activation='relu'))\n",
    "cnn.add(Dense(10,activation='softmax'))\n",
    "cnn.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b56fc95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 0.0538 - accuracy: 0.9835 - val_loss: 0.0325 - val_accuracy: 0.9893\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0330 - accuracy: 0.9894 - val_loss: 0.0201 - val_accuracy: 0.9937\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 0.0109 - val_accuracy: 0.9968\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.0170 - accuracy: 0.9946 - val_loss: 0.0117 - val_accuracy: 0.9962\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 0.0060 - val_accuracy: 0.9979\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0116 - val_accuracy: 0.9960\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0080 - val_accuracy: 0.9971\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.0073 - val_accuracy: 0.9974\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0035 - val_accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(x_train,y_train,epochs = 10, verbose = 1, validation_data=(x_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542fa18",
   "metadata": {},
   "source": [
    "## Image recognition using  CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e1cd7",
   "metadata": {},
   "source": [
    "In this project we will be using CIFAR-10 dataset. This dataset includes thousands of pictures of 10  different kinds of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a72f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this project we will be using CIFAR-10 dataset. This dataset includes thousands of pictures of 10 different kinds of \n",
    "# objects like airplanes, automobiles, birds and so on. Each image in the dataset includes a matching label so we know\n",
    "# what kind of image it is. The images in the CIFAR-10 dataset are only 32x32 pixels. These are very low resolution images.\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Flatten, Conv2D, MaxPooling2D\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65dff7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8985671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "(X_train,y_train),(X_test, y_test)= cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aa4ba920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "\n",
    "X_train= X_train.astype('float32')\n",
    "X_test=X_test.astype('float32')\n",
    "X_train /=255.0\n",
    "X_test /=255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2a8b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "\n",
    "y_train= to_categorical(y_train,10)\n",
    "y_test= to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3e4916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_42 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 15, 15, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 512)               1180160   \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model and add layers\n",
    "\n",
    "# Dropout- The idea is that between certain layers, we will randomly throw away some of the data ny cutting some of the\n",
    "# connections between the layers. This is called as dropuout. Usually, we will add dropuout right after max pooling layers\n",
    "# or after a group of dense layers.\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3), padding='same', input_shape=(32,32,3), activation='relu'))\n",
    "model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "#Print the summary fo the model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a08bf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 76s 48ms/step - loss: 1.5076 - accuracy: 0.4482 - val_loss: 1.2062 - val_accuracy: 0.5636\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 1.1427 - accuracy: 0.5950 - val_loss: 0.9624 - val_accuracy: 0.6599\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.9985 - accuracy: 0.6485 - val_loss: 0.8567 - val_accuracy: 0.7015\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 0.9009 - accuracy: 0.6853 - val_loss: 0.8288 - val_accuracy: 0.7167\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 76s 49ms/step - loss: 0.8420 - accuracy: 0.7040 - val_loss: 0.7788 - val_accuracy: 0.7323\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 0.7951 - accuracy: 0.7229 - val_loss: 0.7926 - val_accuracy: 0.7302\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 0.7549 - accuracy: 0.7343 - val_loss: 0.7623 - val_accuracy: 0.7376\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 78s 50ms/step - loss: 0.7204 - accuracy: 0.7461 - val_loss: 0.7304 - val_accuracy: 0.7482\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 0.6993 - accuracy: 0.7527 - val_loss: 0.7030 - val_accuracy: 0.7585\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 79s 50ms/step - loss: 0.6736 - accuracy: 0.7616 - val_loss: 0.6958 - val_accuracy: 0.7659\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 77s 50ms/step - loss: 0.6560 - accuracy: 0.7715 - val_loss: 0.8386 - val_accuracy: 0.7091\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 0.6381 - accuracy: 0.7751 - val_loss: 0.7258 - val_accuracy: 0.7581\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 0.6240 - accuracy: 0.7813 - val_loss: 0.6560 - val_accuracy: 0.7771\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6103 - accuracy: 0.7865 - val_loss: 0.7107 - val_accuracy: 0.7589\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 87s 56ms/step - loss: 0.5903 - accuracy: 0.7911 - val_loss: 0.6484 - val_accuracy: 0.7804\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 83s 53ms/step - loss: 0.5761 - accuracy: 0.7975 - val_loss: 0.6641 - val_accuracy: 0.7764\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 81s 52ms/step - loss: 0.5709 - accuracy: 0.7984 - val_loss: 0.6927 - val_accuracy: 0.7696\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.5560 - accuracy: 0.8046 - val_loss: 0.6593 - val_accuracy: 0.7799\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 78s 50ms/step - loss: 0.5483 - accuracy: 0.8072 - val_loss: 0.6481 - val_accuracy: 0.7820\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 85s 54ms/step - loss: 0.5420 - accuracy: 0.8104 - val_loss: 0.6537 - val_accuracy: 0.7824\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 76s 49ms/step - loss: 0.5213 - accuracy: 0.8166 - val_loss: 0.6913 - val_accuracy: 0.7656\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 73s 47ms/step - loss: 0.5255 - accuracy: 0.8149 - val_loss: 0.6450 - val_accuracy: 0.7857\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 0.5169 - accuracy: 0.8183 - val_loss: 0.6635 - val_accuracy: 0.7836\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 0.5135 - accuracy: 0.8219 - val_loss: 0.6510 - val_accuracy: 0.7851\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 73s 47ms/step - loss: 0.5033 - accuracy: 0.8238 - val_loss: 0.6332 - val_accuracy: 0.7932\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 74s 47ms/step - loss: 0.4940 - accuracy: 0.8279 - val_loss: 0.6797 - val_accuracy: 0.7833\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 74s 48ms/step - loss: 0.4928 - accuracy: 0.8264 - val_loss: 0.7105 - val_accuracy: 0.7788\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 74s 47ms/step - loss: 0.4821 - accuracy: 0.8318 - val_loss: 0.7160 - val_accuracy: 0.7723\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 74s 47ms/step - loss: 0.4837 - accuracy: 0.8329 - val_loss: 0.6671 - val_accuracy: 0.7790\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 74s 47ms/step - loss: 0.4718 - accuracy: 0.8343 - val_loss: 0.6832 - val_accuracy: 0.7812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe97a29340>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(X_test,y_test),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "856805bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4411"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the neural network structure\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure.json\")\n",
    "f.write_text(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "086fd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained neural network weights\n",
    "    \n",
    "model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38822c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions \n",
    "from  keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ac145f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset class labels\n",
    "\n",
    "class_labels = ['Planes','Car','Bird','Cat','Deer','Dog','Frog','Horse','Boat','Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "512444f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json file that contains the model structure\n",
    "\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3045bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the Keras model object from the json data\n",
    "model = model_from_json(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb586737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model trained weights\n",
    "model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1a04dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image file to test\n",
    "img= image.load_img('frog.png',target_size = (32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4a2569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a numpy array\n",
    "\n",
    "image_to_test = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "375d5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a fourth dimension, since keras expects a list of images, not a single image\n",
    "list_of_images = np.expand_dims(image_to_test,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f9383be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using the model\n",
    "\n",
    "results = model.predict(list_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f17c9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are only testin one image, we only need to check the first result\n",
    "\n",
    "single_result = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "14347946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get a likelihood score for all 10 possible classes. Find out which class has the highest\n",
    "# score\n",
    "most_likely_class_index = int(np.argmax(single_result))\n",
    "class_likelihood = single_result[most_likely_class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7ee99d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the most likely class\n",
    "class_label = class_labels[most_likely_class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "456a4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is image is a Planes - Likelihood: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Printing the result\n",
    "print('This is image is a {} - Likelihood: {:2f}'.format(class_label, class_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687b98e",
   "metadata": {},
   "source": [
    "###    Pretrained VGG16 neural network model for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df89d4",
   "metadata": {},
   "source": [
    "Sometimes instead of using own NN designs fro scratch, it makes sense to reuse an existing neural network design as a starting point for projects. These pre_trained models are trained on large data sets. So, we can use those pretrained models and either use them directly or use them as a starting point for our own training ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12199e0b",
   "metadata": {},
   "source": [
    "image recognition models included in keras are all trained to recognise images from the imageNet dataset\n",
    "\n",
    "ImageNet dataset is a collection of millions of pictures of objects that have been labelled so that you can use them to train computers to recognise those objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf06d5ea",
   "metadata": {},
   "source": [
    "Lets talk about the NN designs included with Keras that we can use\n",
    "\n",
    "VGG - VGG is a deep NN with either 16 or 19 layers. Its a very standard CNN design. Its still widely used as a basis for other models because its easy to work with and easy to understand.\n",
    "\n",
    "ResNet-50 - it was devleped in 2015. It is a 50 layer neural network that manages to be more accurate and uses less memory than VGG. \n",
    "\n",
    "Inception V3 is another design fro 2015 that also performs very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "28f7b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "52657010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 74s 0us/step\n",
      "553476096/553467096 [==============================] - 74s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load keras vgg16 model that was pretrained against the imageNet database\n",
    "\n",
    "model = vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4dc1f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(\"bay.jpeg\",target_size=(224,224))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2f444107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver the image to numpy array\n",
    "x = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fc100178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fourth dimension since keras expects a list of images\n",
    "\n",
    "x = np.expand_dims(x,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7bde09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the input imags pixel values to the range used when training the model\n",
    "x = vgg16.preprocess_input(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "664bd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the impage throught the  deep NN to make a prediction\n",
    "\n",
    "predictions = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f7df231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 1us/step\n",
      "49152/35363 [=========================================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Look up the names of the predicted classes. Index zero is the results for the first image\n",
    "predicted_classes = vgg16.decode_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "308d4589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Predictions for this image:\n",
      "prediction: seashore - 0.395213\n",
      "prediction: promontory - 0.326128\n",
      "prediction: lakeside - 0.119613\n",
      "prediction: breakwater - 0.062801\n",
      "prediction: sandbar - 0.045267\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Predictions for this image:\")\n",
    "for imagenet_id,name,likelihood in predicted_classes[0]:\n",
    "    print(\"prediction: {} - {:2f}\".format(name,likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0dc766",
   "metadata": {},
   "source": [
    "Transfer Learning as an alternative to training a neural network.\n",
    "\n",
    "It is where you take a model trained on one set of data and then use the knowledge it learned to give it a headstart when solving a new problem.\n",
    "\n",
    "Transfer learning is also very useful when you only have a small training dataset. If you only have a few hundred training images for your image recognition system, you don't have enough data to teach your model from skratch, so it makes sense to start with a model trained for something else and adapt it to your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9433a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6d37212d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ece0f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noice\n"
     ]
    }
   ],
   "source": [
    "print(\"noice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4449e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
