{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924842b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving Classification Problems in ML\n",
    "\n",
    "# Classification problems are the type of problems where you have to predict a dicrete value, i.e. whether the condition of car\n",
    "# is good or not. Whether or not a student will pass an exam.\n",
    "\n",
    "# Preparing Data for classification problems\n",
    "\n",
    "# Like regression, you have to convert data into a specific format before it can be used to train classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn Modelling\n",
    "\n",
    "churn_df = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "# This dataset contains records of customers who left the bank six months after various information about them is recorded.\n",
    "\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4011517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output shows that the dataset contains information such as surname, customer id, geography, gender etc. The\n",
    "# Exited column contains information about whether or not the customer exited the bank after six months\n",
    "\n",
    "# We do not need rownumer, customerid, surname\n",
    "\n",
    "# removing unnecessary columns\n",
    "\n",
    "churn_df= churn_df.drop(['RowNumber','CustomerId','Surname'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64118c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing data into features and labels\n",
    "\n",
    "# The features set i.e. X contains all the columns except te Exited column. On the other hand, the label set 'y' contains\n",
    "# values from the Exited column only\n",
    "\n",
    "# create feature set\n",
    "X=churn_df.drop(['Exited'],axis=1)\n",
    "\n",
    "#create label set\n",
    "y=churn_df['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178929bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical data to numbers\n",
    "# For classification problems too we need to convert categorical columns to numerical ones\n",
    "\n",
    "# dropping categorical columns\n",
    "numerical=X.drop(['Geography','Gender'],axis=1)\n",
    "\n",
    "numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b82369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering categorical columns\n",
    "\n",
    "categorical= X.filter(['Geography','Gender'])\n",
    "categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cf547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting categorical to numeric\n",
    "\n",
    "# converting categorical columns to one hot encoded columns\n",
    "\n",
    "import pandas as pd\n",
    "cat_numerical= pd.get_dummies(categorical, drop_first=True)\n",
    "cat_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concating numerical columns with one-ot encoded columns\n",
    "\n",
    "X= pd.concat([numerical,cat_numerical],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the dataset into Training and Test sets\n",
    "\n",
    "# Like regression in classification problems we divide the dataset into two sets i.e. train set and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datascaling/Normalization- The last step before the data is passed to the machine learning algorithm is to scale the data\n",
    "\n",
    "# applying standard scaling to the dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "\n",
    "X_train= sc.fit_transform(X_train)\n",
    "X_test= sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092409e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have converted data into a format that can be used to train machine learning algorithms for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification problems- These are those classification problems where there re only two possible values for\n",
    "# the output label/ E.g. whether a student will pass or fail. Whther a customer will churn or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression- Logistic regression is a linear model, which makes classification by passing the output of\n",
    "# linear regression through a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45eeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf= LogisticRegression()\n",
    "\n",
    "# training the logistc regression classifier\n",
    "classifier= log_clf.fit(X_train, y_train)\n",
    "\n",
    "#making predictions on the test set\n",
    "y_pred= classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3caf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have trained a model and have made predictions on the test set, the next step is to know how well your model\n",
    "# has performed for making predictions on the unknown test set. There are various metrics to evaluate a classification model.\n",
    "# These are F1 score, recall, precision, accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45791816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Negatives: (TN/tn): True negatives are those output labels that are actually false and the model also predicted\n",
    "# as false\n",
    "# True Positive: True Positives are those labels that are actually true and also predicted as true by the model\n",
    "# False Negative: False negatives are labels that are actually true but predicted as false by ML models\n",
    "# False Positive: Labels that are actually false but predicted as true by the model are called false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3900b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision- Another way to analyse a classification algorithm which is basically obtained by dividing true positives\n",
    "# by the sum of true positives and false positive\n",
    "\n",
    "# Recall- Recall is calculated by dividing true positives by the sum of true positive and false negative\n",
    "\n",
    "# F1-measure- Harmonic mean of precision and recall\n",
    "\n",
    "# Accuracy- It refers to the number of correctly predicted labels divided by the total number of observations in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855e66c",
   "metadata": {},
   "source": [
    "The output shows that for 81 percent of the records in the test set, logistic regression correctly predicted whether or not a customer will leave the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259565b",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf9670",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    " It is a tree based algo which converts features to tree nodes and uses entropy losses to make classification predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42,n_estimators = 500)\n",
    "\n",
    "# training the random forest classifier\n",
    "classifier = rf_clf.fit(X_train,y_train)\n",
    "\n",
    "# making predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#evaluating the algo on the test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering- Clustering algorithms are unsupervised algorithms where the training data is not labeled.\n",
    "# The algorithm cluster or group the data sets based on the common characteristics.\n",
    "# K-Means clustering\n",
    "# Hierarchical clustering\n",
    "\n",
    "#K-means clustering- is the most commonly used algorithm for clustering unlabeled data. In K-Means clustering, K refers to the\n",
    "# number of clusters that you want your data to be grouped into.\n",
    "\n",
    "# Steps for K-means clustering\n",
    "\n",
    "# Randomly assign centroid values for each cluster\n",
    "# Calculate the distance (Euclidean or Manhattan) between each data point and centroid values of all clusters\n",
    "# Assign the data point to the cluster of the centroid with the shortest distance\n",
    "# Calculate and update the centroid values based on the mean values of the coordinates of all the data points\n",
    "# of the corresponding cluster.\n",
    "# Repeat steps 2-4 util new centroid values for all the clusters are different from the previous centroid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4717b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why K-Means clustering\n",
    "\n",
    "# K-Means is simple to implement algorithm\n",
    "# It can be applied to large datasets\n",
    "# It scales well to unseen data points\n",
    "# It generalises well to clusters of various sizes and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Segmentation using K-Means clustering\n",
    "\n",
    "# In this example you will see how to segment customers based on their incomes and past spending habits. You will then \n",
    "# identify customers who have high incomes and higher spending\n",
    "\n",
    "import  numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c33228",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Mall_Customers.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above data the spending score is the score assigned to customers based on their previous spending habits.\n",
    "# Customers with higher spending in the past have higher scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ea2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dataset['Annual Income (k$)'],kde = True,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465000fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot a regression line between annual income and spending score to see if there is any linear relationship\n",
    "# between the two or not\n",
    "\n",
    "# plotting regression plot for annual income against spending score\n",
    "sns.regplot(x='Annual Income (k$)',y=\"Spending Score (1-100)\",data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e1cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above chart there is no linear relationship between annual income and spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='Age',y=\"Spending Score (1-100)\",data = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a641cd",
   "metadata": {},
   "source": [
    "The above chart confirms an inverse linear relationship between age and spending score. It can be inferred from the output that young people have higher spending compared to older people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae247b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(['Annual Income (k$)','Spending Score (1-100)'],axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing kmeans clustering\n",
    "km_model = KMeans(n_clusters=4)\n",
    "km_model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef881e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Once the model is trained, you can print the cluster centers \n",
    "print(km_model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to finding cluster centers, the K_Means class also assigns a cluster label to each data point. The cluster labels\n",
    "# are numbers that basically serve as cluster id 0,1,2,3\n",
    "print(km_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dataset.values[:,0],dataset.values[:,1],c = km_model.labels_,cmap='rainbow')\n",
    "plt.scatter(km_model.cluster_centers_[:,0], km_model.cluster_centers_[:,1], s=100, c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b5a03",
   "metadata": {},
   "source": [
    "Till now we have been randomly initialising the value of k or the number of cluster. However, we do not know how many segments of customers are there in out dataset. To find the optimal number of customer segments, we need to find the optimal number of K. This can be done using a method know as elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=[]\n",
    "for i in range(1,11):\n",
    "    km = KMeans(n_clusters=i).fit(dataset)\n",
    "    loss.append(km.inertia_)\n",
    "    \n",
    "# printing loss against number of clusters\n",
    "plt.plot(range(1,11),loss)\n",
    "plt.title(\"Findind optimal clusters using Elbow Method\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above output, it can be seen that the value of inertia didn't decrease much after five clusters\n",
    "\n",
    "# Lets segment our customer data into five groups by creating five clusters\n",
    "km_model = KMeans(n_clusters=5)\n",
    "km_model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9de132",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dataset.values[:,0], dataset.values[:,1], c=km_model.labels_, cmap='rainbow')\n",
    "\n",
    "plt.scatter(km_model.cluster_centers_[:,0], km_model.cluster_centers_[:,1], s=100, c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63455d18",
   "metadata": {},
   "source": [
    "From the above output, you can see that the customers are divided into five segements. The customers in the middle of the plot are the customers with average income and average spending. The customers beloging to the red cluster are the ones with low income and low spending. \n",
    "\n",
    "\n",
    "We need to target the customers belogning to the top right cluster. These are the customers with high incomes and higher spending and they are more likely to spend in the future. \n",
    "\n",
    "So, any new marketing campaigns or\n",
    "advertisements should be directed to these cusomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will lot the centers of the clusters\n",
    "\n",
    "print(km_model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953903ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fetch all the record of customer with id 1\n",
    "cluster_map = pd.DataFrame()\n",
    "cluster_map['data_index'] = dataset.index.values\n",
    "cluster_map['cluster'] = km_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_map = cluster_map[cluster_map.cluster==1]\n",
    "cluster_map.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can simply filter all the records from the cluster_map dataframe, where the vlaue of the cluster column is 1\n",
    "\n",
    "cluster_map = cluster_map[cluster_map.cluster==1]\n",
    "cluster_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction with PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis)\n",
    "# SVD (Singular Value Decomposition)\n",
    "\n",
    "# Dimensionality reduction refers to reducing the number of features in a dataset in such a way that the overall\n",
    "# performance of the algorithms trained on the dataset is minimally affected.With dimensionality reduction, the training\n",
    "# time of the statistical algorithm can be significantly reduced and data can be visualised more easily since it is not easy to \n",
    "# visualise datasets in higher dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906b8f9",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d36d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is an unsupervised dimensionality reduction technique that doesn't depends on the labels of the dataset. \n",
    "# PCA prioritizes features on the basis of their ability to cause maximum variance in the outut.\n",
    "# The idea behind PCA is to capture those features that conatin maximum information about the datsset.\n",
    "\n",
    "# Why use PCA?\n",
    "\n",
    "# Using PCA correlated features can be detected and removed\n",
    "# It reduces overfitting because of a reduction in the number of features\n",
    "# Model training can be expedited\n",
    "\n",
    "#Disadvantages of PCA\n",
    "\n",
    "# You need to standardize the data before you apply PCA\n",
    "# Independent variables becomed less integrable\n",
    "# Some amount of information is lost when you reduce features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31482ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df= sns.load_dataset(\"iris\")\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating feature sets\n",
    "X = iris_df.drop(['species'],axis=1)\n",
    "\n",
    "# creating labels set\n",
    "y = iris_df['species']\n",
    "\n",
    "# converting label to number\n",
    "from sklearn import preprocessing\n",
    "le=preprocessing.LabelEncoder()\n",
    "y=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Before we apply PCA on a dataset, we will divide it into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying scaling on training and test data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PCA class\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# creating object of PCA class\n",
    "pca = PCA()\n",
    "# training the PCA model on training data\n",
    "X_train = pca.fit_transform(X_train)\n",
    "\n",
    "# make predictions on the test data\n",
    "X_test = pca.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ccb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing variance ratio\n",
    "variance_ratios = pca.explained_variance_ratio_\n",
    "print(variance_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9211e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06192e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(X_train,y_train)\n",
    "y_pred = lg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ddb58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:,0],X_test[:,1], c = y_test, cmap = 'rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db26cb0",
   "metadata": {},
   "source": [
    "Performance of the ml model depends upon several factors such as training and test sets used to train and test an algorithm, values of the hyperparameters of an algorithm and metrics used to evaluate the performance  of an algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316f37d",
   "metadata": {},
   "source": [
    "1) K-fold cross validation\n",
    "\n",
    "It is a data splitting technique that lets you train and test the model on all subsets of data \n",
    "\n",
    "Till now we used 80:20 split.ie,. 80% is train and 20% is test. For more stable results it is recommended that all the parts of the dataset are atleast used once for training and once for testing. To do so , K-fold cross validation can be used.\n",
    "\n",
    "With K-fold cross validation the data is divided into K parts. The experiments are also performed for k parts.In each experiment K-1 parts are used for training and the kth part is used for testing \n",
    "\n",
    "for example in 5-fold cross validation, the data is divided in to 5 equal parts. In the first iteration, K1-K4 are used for training  while K5 is used for testing. In the second test, K1, K2, K3 , K5 are used for training while K4 is used for testing.\n",
    "\n",
    "This way each part is used atleast once for testing and once for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2062174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction without cross validation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4274bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = pd.read_csv(\"winequality-red (1).csv\")\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba61656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_data.drop(['quality'],axis = 1)\n",
    "y = wine_data['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test  = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82aa31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=42,n_estimators = 500)\n",
    "regressor = rf_reg.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Mean Absolute Error:\", metrics.mean_absolute_error(y_test,y_pred))\n",
    "print(\"Mean Squared Error:\",metrics.mean_squared_error(y_test,y_pred))\n",
    "print(\"Root Mean Squared Error\",np.sqrt(metrics.mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a726f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using cross_validation - this time data will be trained by using 5-fold validation\n",
    "\n",
    "X = wine_data.drop(['quality'],axis = 1)\n",
    "y = wine_data['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, instead of dividing the dataset into the training and test sets, you will initialize the instance of your ML model\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state = 42, n_estimators = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#applying cross validation with 5 folds\n",
    "\n",
    "score = cross_val_score(rf_reg,X,y,cv=5,scoring = 'neg_mean_absolute_error')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2af8a",
   "metadata": {},
   "source": [
    "Hyperparameter selection - there exists algorithms that can be used to select the best paramters from a list of parameters.\n",
    "\n",
    "One such algorithm is Grid Search algorithm\n",
    "\n",
    "To perform grid search, you have to create a dictionary where dictionary keys represent the parameter name and dictionary values consist of lists, which contains items that contain values that you want to test for the attributes specified as dictionary keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb024bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the list of hyperparameters to be tested\n",
    "\n",
    "grid_param = {\n",
    "    'n_estimators':[100,200,300,400,500],\n",
    "    'min_samples_leaf':[1,3,5],\n",
    "    'bootstrap':[True,False],\n",
    "    'criterion':['mae']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gd_sr = GridSearchCV(estimator=rf_reg,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring = \"neg_mean_absolute_error\",\n",
    "                     cv=5,\n",
    "                     n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with grid search\n",
    "gd_sr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b026b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the grid search finishes training, you can find the best parameters that your grid seach algo selected\n",
    "\n",
    "best_parameters = gd_sr.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74470475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the best case mean absolute error\n",
    "\n",
    "best_results = gd_sr.best_score_\n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07e436",
   "metadata": {},
   "source": [
    "## Spam Email Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b6e9f",
   "metadata": {},
   "source": [
    "Spam email filtering in NLP task where based on the text of the email, we have to classify whether or not an email is a spam email. In this we use supervised ML algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbab03d",
   "metadata": {},
   "source": [
    "The naive bayes algo is on of the supervised ML algorithms that have been proven effective for spam email detection\n",
    "\n",
    "Advanatages\n",
    "- It performs well there is no relationship between attributes in a feature vector\n",
    "- It requires a very small amount of data training\n",
    "- It is very easy to imlement and understand\n",
    "\n",
    "Disadvantages\n",
    "- It is unable to capture the relatioship btw various features in the dataset\n",
    "- If a category exists in the test set but not in the training set. the probability of prediciton for that category in the test set will be set to 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b479d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291af871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_dataset = pd.read_csv(\"emails.csv\")\n",
    "message_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e98a56",
   "metadata": {},
   "source": [
    "    In the above dataset, the text column contains text of emails and the spam column contains the label 1 or 0, where 1 corresponds to spam emails and 0 corresponds to non-spam emails or ham e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83521392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5728, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0edf4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='spam'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data visualization \n",
    "# Plot the pie chart\n",
    "\n",
    "message_dataset.spam.value_counts().plot(kind = 'pie',autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766f6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the above pie chart you can see that 24 percent of th emails are spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf94120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we ll remove stopwords as a,is,you,I, are,etc because these words these occur a lot and they do not have any significant\n",
    "# classification ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49b5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2504a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words from messages\n",
    "message_dataset['text_without_sw'] = message_dataset['text'].apply(lambda x :\" \".join([item for item in x.split()\n",
    "                                                                                      if item not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade4e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we actually train our ML algorithm we need to remove special characters and numbers from our text. Removing special\n",
    "# characters and numbers creates empty spaces in the text, which also needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23aaeb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating feature set\n",
    "X = message_dataset['text']\n",
    "\n",
    "# creating Label set\n",
    "y = message_dataset['spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb5ae3",
   "metadata": {},
   "source": [
    "We will create a function clean_text() which accepts text string and returns a string that is cleaned of digits, special characters and multiple empty spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9835136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning feature set (email messages)\n",
    "def clean_text(doc):\n",
    "    # removing everything except capital and small letters\n",
    "    document = re.sub('[^a-zA-Z]',' ',doc)\n",
    "    \n",
    "    # removing the single characters\n",
    "    document = re.sub(r\"\\s+[a-zA-Z]\\s+\",' ',document)\n",
    "    \n",
    "    # removing the multiple empty spaces\n",
    "    document = re.sub(r'\\s+',' ',document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27401afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sentences =  []\n",
    "reviews = list(X)\n",
    "\n",
    "# cleaning all sentences using clean_text() function\n",
    "\n",
    "for rev in reviews:\n",
    "    X_sentences.append(clean_text(rev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d08f9",
   "metadata": {},
   "source": [
    "convert text into numbers\n",
    "\n",
    "Naive bayes algo is a statistical algo which works with numbers. Therefore we need to convert the text of e-mails into numeric form.\n",
    "\n",
    "There are various ways:\n",
    "- Bag of words\n",
    "- TFIDF\n",
    "- Word Embedding\n",
    "\n",
    "In this project we ll use TFIDF technique  for converting text to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3334b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stopwords and TFIDF vectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20b3472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting e-mail messages to text via TFIDF vecoriser\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2500,min_df=5,max_df=0.7,\n",
    "                            stop_words=stopwords.words('english'))\n",
    "\n",
    "X = vectorizer.fit_transform(X_sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130f923",
   "metadata": {},
   "source": [
    "x In the above code max_feature attribute specifies that a maximum pf 2500 most occuring keywords should be used to create a feature dictionary.\n",
    " \n",
    " The min_df attribute here specifies to only include words that occur in atleast 5 documents\n",
    " \n",
    " The max_df attribute defines not to include words that occur in more than 70 percent of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8810113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test  =  train_test_split(X,y,test_size=0.20,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03cce9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the Naive Bayes algo on the training data\n",
    "spam_detector = MultinomialNB()\n",
    "spam_detector.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ab8c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model performance\n",
    "\n",
    "y_pred = spam_detector.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5bda7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[849   7]\n",
      " [ 18 272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       856\n",
      "           1       0.97      0.94      0.96       290\n",
      "\n",
      "    accuracy                           0.98      1146\n",
      "   macro avg       0.98      0.96      0.97      1146\n",
      "weighted avg       0.98      0.98      0.98      1146\n",
      "\n",
      "0.9781849912739965\n"
     ]
    }
   ],
   "source": [
    "# Evaluating  model performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows that the model is 97.81 % accurate while predicting whether the msg is spam or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3bd7262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject localized software all languages available hello we would like to offer localized software versions german french spanish uk and many others aii iisted software is available for immediate downioad no need to wait week for cd deiivery just few exampies norton lnternet security pro windows xp professionai with sp fuil version corei draw graphics suite dreamweaver mx homesite inciudinq macromedia studio mx just browse our site and find any software you need in your native ianguaqe best reqards kayieen \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# making predictions on single instance\n",
    "\n",
    "# printing senctence at index 56 in the dataset\n",
    "\n",
    "print(X_sentences[56])\n",
    "\n",
    "# print label for the sentence at index 56\n",
    "\n",
    "print(y[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6b46d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# making prediction for the label for the sentence at index 56\n",
    "\n",
    "print(spam_detector.predict(vectorizer.transform([X_sentences[56]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04064f8",
   "metadata": {},
   "source": [
    "The model correctly classified the message as spam "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3e315",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2595d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "956b1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11b8f153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "376140c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13448/408228920.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# printing the first image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# printing the first image\n",
    "image_0 = mnist.data[0].reshape(28,28)\n",
    "import matplotlib as plt\n",
    "plt.gray()\n",
    "plt.matshow(image_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25383f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
